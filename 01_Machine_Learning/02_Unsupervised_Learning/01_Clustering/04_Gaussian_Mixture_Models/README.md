# 04_Gaussian_Mixture_Models

A Gaussian Mixture Model (GMM) is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions with unknown parameters. It's a "soft" clustering algorithm, meaning it can assign a probability of a data point belonging to each cluster.

### How it works:

Instead of assigning each point to a single cluster (like k-Means), GMMs compute the probability of each point belonging to each of the `k` Gaussian distributions.

The algorithm uses the **Expectation-Maximization (EM)** algorithm to find the parameters of the Gaussian distributions:
1.  **Initialization**: Initialize the parameters (mean, covariance, and mixing coefficient) for each Gaussian distribution.
2.  **Expectation (E-step)**: For each data point, calculate the probability that it was generated by each of the component Gaussians.
3.  **Maximization (M-step)**: Update the parameters of each Gaussian to maximize the likelihood of the data given these probabilities.
4.  **Repeat**: Repeat the E and M steps until the model converges.

### Advantages over k-Means:

-   **Soft Clustering**: Provides probabilities instead of hard assignments.
-   **Cluster Shape**: Can handle non-circular, ellipsoidal clusters due to the use of covariance matrices. 